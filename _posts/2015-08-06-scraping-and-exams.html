---
layout: post
title: Scraping and Exams
date: 2015-08-06 05:50:43.000000000 +05:30
type: post
published: true
status: publish
categories: []
tags: []
meta:
  _rest_api_published: '1'
  _rest_api_client_id: '-1'
  _publicize_job_id: '13452021151'
author:
  login: sparshithsampath
  email: sparshithsampath@gmail.com
  display_name: sparshithsampath
  first_name: ''
  last_name: ''
---
<p>The last post about scraping was on the stats page of nba.com,where I used the api the page was using to conduct client-side rendering to extract data. This one is for a site that uses server-side rendering.</p>
<p>I wrote this code to extract results of all years of all departments from the results page. Once you run the code, the results will be saved as .csv files in respective folders.</p>
<p>Notes:</p>
<ol>
<li>Dependencies: requests, BeautifulSoup, urllib2, csv, os.</li>
<li>Be nice. Execute the code a little while after results have been announced. Ideally, in multiple batches.Â  I have heard that 4000 requests aren't too kind to the servers that they are using.</li>
</ol>
<p>[sourcecode langauge="python"]</p>
<p>import requests<br />
from bs4 import BeautifulSoup<br />
from urllib2 import urlopen<br />
import csv<br />
import os</p>
<p>def ensure_dir(f):			# function to create directories for storing data<br />
    d = os.path.dirname(f)<br />
    if not os.path.exists(d):<br />
        os.makedirs(d)</p>
<p>#headers to be sent to url which has the results.</p>
<p>headers = {<br />
	'Accept ':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',<br />
'Accept-Encoding':'gzip, deflate, sdch',<br />
'Accept-Language': 'en-US,en;q=0.8',<br />
    'Cache-Control':'max-age=1000000000',<br />
    'Cookie':'PHPSESSID=37b9f16874c03d7c07789c4c5238a641; mypets=0c; __utma=50120421.130946855.1435211323.1435312969.1435635572.3;    __utmb=50120421.2.10.1435635572; __utmc=50120421; __utmz=50120421.1435211323.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)',<br />
    'Connection':'keep-alive',<br />
   'Host': 'www.rvce.edu.in',<br />
   'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36'<br />
}</p>
<p># headers to be sent to base_url.</p>
<p>headers_base={<br />
'Host': 'www.rvce.edu.in',<br />
'Connection': 'keep-alive',<br />
'Content-Length': '25',<br />
'Cache-Control': 'max-age=0',<br />
'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',<br />
'Origin': 'http://www.rvce.edu.in',<br />
'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36',<br />
'Content-Type': 'application/x-www-form-urlencoded',<br />
'Referer': 'http://www.rvce.edu.in/results/',<br />
'Accept-Encoding': 'gzip, deflate',<br />
'Accept-Language': 'en-US,en;q=0.8',<br />
'Cookie': 'PHPSESSID=37b9f16874c03d7c07789c4c5238a641; mypets=0c; __utmt=1; __utma=50120421.130946855.1435211323.1435887704.1435890820.12; __utmb=50120421.4.10.1435890820; __utmc=50120421; __utmz=50120421.1435211323.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)'<br />
}</p>
<p>cur_acad_year= 14                                 #last two digits of the current academic year</p>
<p>#getting a list of departments and their id's.<br />
depts=[]<br />
base_url= &quot;http://www.rvce.edu.in/results/&quot;<br />
q= requests.get(base_url)<br />
soup= BeautifulSoup( q.text, &quot;lxml&quot;)<br />
rows= soup.findAll(&quot;tr&quot;)<br />
options= rows[3].findAll(&quot;option&quot;)<br />
a=0<br />
for option in options:<br />
	if(a&amp;gt;0):                                                  # First option tag just contains header<br />
		depts.append([option['value'],option.string])<br />
	else:<br />
		a= a+1</p>
<p>test=0</p>
<p>for department in depts:<br />
	sem_value = 2                                          #Change this value to 1 if odd sem, 2 if even sem<br />
	department_name= department[1].strip()</p>
<p>	if(test != 0):<br />
		break<br />
	print &quot;Downloading data for the department &quot;+ department_name+ &quot;....&quot;<br />
	for w in range(0,4):<br />
		value= str(department[0])<br />
		s= requests.post(base_url, data={'branch':value, 'sem': sem_value ,'Submit':'go'},headers= headers_base)<br />
		print &quot;Accessing data from usn: &quot;+s.url+ &quot;.....&quot;</p>
<p>		url= s.url<br />
		html= urlopen(url). read()<br />
		soup = BeautifulSoup(html, &quot;lxml&quot;)</p>
<p>		&quot;&quot;&quot;<br />
		method to access elements using bs4</p>
<p>		table= soup.findAll(&quot;table&quot;)<br />
		row= table[0].findAll(&quot;tr&quot;)<br />
		cell= row[3].findAll(&quot;td&quot;)<br />
		branch= cell[1].string<br />
		&quot;&quot;&quot;	</p>
<p>		sem = sem_value<br />
		# for writing in to csv files<br />
		cur_dir= os.path.dirname(os.path.abspath(__file__))<br />
		project_name= &quot;RVCE_Results_Anaysis&quot;<br />
		dir_parent= department_name<br />
		dir_child= department_name+&quot;_&quot;+str(sem)<br />
		filename= department_name+&quot;_&quot;+str(sem)+&quot;_sem.csv&quot;<br />
		f= cur_dir+ &quot;/&quot;+ project_name+ &quot;/&quot; + dir_parent+ &quot;/&quot;+ dir_child+ &quot;/&quot;+ filename<br />
		ensure_dir(f)</p>
<p>		#Headers of csv: USN,Name,Branch, [Subject ID's], SGPA<br />
		csv_headers= ['USN', 'Name', 'Branch']<br />
		table= soup.findAll(&quot;table&quot;)<br />
		rows= table[1].findAll(&quot;tr&quot;)<br />
		for row in rows:<br />
		     cells= row.findAll(&quot;td&quot;)<br />
		     if(len(cells)==2):<br />
		     	csv_headers.append(cells[0].string)</p>
<p>		ofile  = open(f, &quot;wb&quot;)<br />
		writer = csv.writer(ofile, delimiter='\t', quotechar='&quot;', quoting=csv.QUOTE_ALL)<br />
		writer.writerow(csv_headers)</p>
<p>		#Generating USN's<br />
		if( department_name == &quot;Information Science&quot;):<br />
			branch_code= &quot;IS&quot;<br />
		elif( department_name == &quot;Instrumentation Technology&quot;):<br />
			branch_code= &quot;IT&quot;<br />
		elif( department_name == &quot;Bio Technology&quot;):<br />
			branch_code= &quot;BT&quot;<br />
		elif(department_name == &quot;B. Arch&quot;):<br />
			branch_code= &quot;AT&quot;<br />
		usn= &quot;1RV&quot;<br />
		if( sem%2 ==0 ):                           #even sem<br />
			year= cur_acad_year- (sem/2) +1</p>
<p>		elif( sem_value%2 ==1):<br />
			year = cur_acad_year- (sem/2)<br />
		usn= usn + str(year)+ branch_code<br />
		for j in range(1, 70):<br />
			if(j&amp;lt;10):<br />
				cur_usn = usn +&quot;00&quot;+ str(j) # can also use cur_usn.zfill(3)<br />
			elif(j&amp;gt;9 and j&amp;lt;100):<br />
				cur_usn = usn +&quot;0&quot;+ str(j)<br />
			else:<br />
				usn = usn + str(j)<br />
			args= {'usn': cur_usn}</p>
<p>		# Getting results<br />
			csv_rows=[]<br />
			r = requests.post( url , data=args, headers=headers)<br />
			soup= BeautifulSoup( r.text, &quot;lxml&quot;)</p>
<p>			#extracting names of students<br />
			divisions= soup.findAll(&quot;div&quot;)<br />
			name= divisions[2].string<br />
			try:<br />
				name = name.replace(u'\xa0', u' ').replace('Name','').strip().replace(':', '').strip().title()	</p>
<p>			except AttributeError:<br />
				print &quot;No student at USN: &quot;,<br />
				print cur_usn<br />
				continue</p>
<p>			print &quot;Entering Data for &quot; +name+ &quot; with USN &quot;+ cur_usn+ &quot;...&quot;<br />
			csv_rows.extend([cur_usn, name, department_name])</p>
<p>			tables= soup.findAll(&quot;table&quot;)<br />
			rows= tables[1].findAll(&quot;tr&quot;)<br />
			for row  in rows:<br />
				cells= row.findAll(&quot;td&quot;)<br />
		     		if(len(cells)==3):<br />
			     		csv_rows.append(cells[2].string.strip())<br />
			     	elif(len(cells)==2):<br />
			     		csv_rows.append(cells[1].findAll(&quot;strong&quot;)[0].string)</p>
<p>			writer.writerow(csv_rows)</p>
<p>		sem_value= sem_value + 2</p>
<p>print &quot;Finishem&quot;<br />
ofile.close()</p>
<p>[/sourcecode]</p>
<p>Drop a mail at sparshithsampath@gmail.com, if you have any questions or suggestions.</p>
