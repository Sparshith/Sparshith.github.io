---
layout: post
title: AI and EY
date: 2015-06-20 09:48:42.000000000 +05:30
type: post
published: true
status: publish
categories: []
tags: []
meta:
  _rest_api_published: '1'
  _rest_api_client_id: '-1'
  _publicize_job_id: '11856274692'
author:
  login: sparshithsampath
  email: sparshithsampath@gmail.com
  display_name: sparshithsampath
  first_name: ''
  last_name: ''
---
<blockquote>We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.</p>
<p>- <em>First paragraph of the Dartmouth Proposal.</em></p></blockquote>
<p>The 1956 Dartmouth Summer Research Project on Artificial Intelligence was a major conference. The organizers consisted of quite a few renowned computer scientists, and all of them had been involved in a lot of related theoretical and practical work, and some of whom had built functioning computers or programming languages - so one can expect them all to have had direct feedback about what was and wasn't doable in computing. But even minus Hindsight bias, they got it quite wrong. The confidence of the Dartmouth was based on the idea that since they informally understood certain concepts and could begin to capture some of this understanding in a formal model, then it must be possible to capture <em>all</em> this understanding in a formal model. This extrapolation is where they went wrong. Today's AI developers realize that cognition has a lot more complexity.  Problems on AI still remain largely unsolved, but the it continues to be extremely fascinating.</p>
<p><em><strong>So what is AI?</strong></em></p>
<ol>
<li><strong>Turing</strong> said "We need not decide if a machine can "think"; we need only decide if a machine can act as intelligently as a human being." According to him, a machine is intelligent is it passes the Turing test. At the risk of diverting from the real issue, I want to expound on one of the most popular versions of the Turing test and more recently, the title of a popular movie; The Imitation game.<br />
<blockquote><p>  There are three players A,B and C. A is male, B is Female and C is of either sex and is the interrogator. All communication between entities is purely text-oriented. C has to correctly identify the sex of A and B. A constantly tries to mislead C, and B tries to guide C to the right answer.</p></blockquote>
<p>This game is played with A as an actual male, and A as a computer in multiple iterations. If the interrogators decide[s] wrongly as often when the game is played [with the computer] as he does when the game is played between a man and a woman, it may be argued that the computer is intelligent.</li>
<li> <strong>The Artificial Brain argument</strong><i>: </i>The brain can be simulated by machines and because brains are intelligent, simulated brains must also be intelligent; thus machines can be intelligent. It's argued that it is technologically feasible to copy the brain directly into hardware and software, and that such a simulation will be essentially identical to the original.</li>
<li><strong>The AI Effect: </strong>Machines are already intelligent, but observers have failed to recognize it. When Deep Blue beat Gary Kasprov in chess, the machine was acting intelligently. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not "real" intelligence after all; thus "real" intelligence is whatever intelligent behavior people can do that machines still can not<strong>.</strong></li>
<li><strong>The Dartmouth Proposal</strong><i>: </i>"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it." This is the one that was discussed in the beginning and what most AI researchers believe in.</li>
</ol>
<p><em><strong>Okay, I understand what AI is, and I concur with one of the definitions, but what in the world is EY?</strong></em></p>
<p>EY, is Eliezer Yudkowsky<b>, </b>is a Machine Learning researcher, writer, blogger, and advocate for FAI(Friendly Artificial Intelligence) and my personal favourite, the author of "Harry Potter and Methods of Rationality", which intends to explain wizadry of the Harry Potter world through scientific method. (Spoiler: Harry gets drafted to Ravenclaw).</p>
<p><strong><em>So how are AI and EY related?</em></strong></p>
<p>Yudkowsky believes in <em>Singularity. </em>According to him, when a sufficiently intelligent smart AI is created, it will have the capability to create even more intelligent AI in the next recursive cycle and this will lead to an exponentially large increase in intelligence with each cycle generating more intelligence in shorter time periods and theoretically will reach infinite intelligence in zero time, which is called the Singularity. But this means that the further iterations of AI will be much smarter than humans and that might be really great for mankind or extremely dangerous. This definition by I.J good captures the essence of it quite beautifully:</p>
<blockquote><p>"Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra-intelligent machine could design even better machines; there would then unquestionably be an "intelligence explosion," and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make."</p></blockquote>
<p>If this premise is accurate, then it means that the creation of the first AI should be done with utmost care. Yudkowsky claims that all research of AI should be focused towards creating "Friendly AI", else it would mean the end of humanity as we know it.</p>
<p><strong><em>But surely, that's crazy. How can a computer ever create any physical change, or possess the capability to eliminate humanity?</em></strong></p>
<p>Yudkowsky argues the following:</p>
<blockquote>
<p id="n75q">In our skulls we carry around 3 pounds of slimy, wet, greyish tissue, corrugated like crumpled toilet paper. You wouldn’t think, to look at the unappetizing lump, that it was some of the most powerful stuff in the known universe. If you’d never seen an anatomy textbook, and you saw a brain lying in the street, you’d say “Yuck!” and try not to get any of it on your shoes. Aristotle thought the brain was an organ that cooled the blood. It doesn’t <em>look</em> dangerous.</p>
<p id="tdba">Five million years ago, the ancestors of lions ruled the day, the ancestors of wolves roamed the night. The ruling predators were armed with teeth and claws – sharp, hard cutting edges, backed up by powerful muscles. Their prey, in self-defense, evolved armored shells, sharp horns, poisonous venoms, camouflage. The war had gone on through hundreds of eons and countless arms races. Many a loser had been removed from the game, but there was no sign of a winner. Where one species had shells, another species would evolve to crack them; where one species became poisonous, another would evolve to tolerate the poison. Each species had its private niche – for who could live in the seas and the skies and the land at once? There was no ultimate weapon and no ultimate defense and no reason to believe any such thing was possible.</p>
<p id="8pa0">Then came the Day of the Squishy Things.</p>
<p id="psa4">They had no armor. They had no claws. They had no venoms.</p>
<p id="ieq4">If you saw a movie of a nuclear explosion going off, and you were told an Earthly life form had done it, you would never in your wildest dreams imagine that the Squishy Things could be responsible. After all, Squishy Things aren’t radioactive.</p>
<p id="i6x4">In the beginning, the Squishy Things had no fighter jets, no machine guns, no rifles, no swords. No bronze, no iron. No hammers, no anvils, no tongs, no smithies, no mines. All the Squishy Things had were squishy fingers – too weak to break a tree, let alone a mountain. Clearly not dangerous. To cut stone you would need steel, and the Squishy Things couldn’t excrete steel. In the environment there were no steel blades for Squishy fingers to pick up. Their bodies could not generate temperatures anywhere near hot enough to melt metal. The whole scenario was obviously absurd.</p>
<p id="m6jy">And as for the Squishy Things manipulating <span class="caps">DNA</span> – that would have been beyond ridiculous. Squishy fingers are not that small. There is no access to <span class="caps">DNA</span> from the Squishy level; it would be like trying to pick up a hydrogen atom. Oh, <em>technically</em> it’s all one universe, <em>technically</em> the Squishy Things and <span class="caps">DNA</span> are part of the same world, the same unified laws of physics, the same great web of causality. But let’s be realistic: you can’t get there from here.</p>
<p id="g9q3">Even if Squishy Things could <em>someday</em> evolve to do any of those feats, it would take thousands of millennia. We have watched the ebb and flow of Life through the eons, and let us tell you, a year is not even a single clock tick of evolutionary time. Oh, sure, <em>technically</em> a year is six hundred trillion trillion trillion trillion Planck intervals. But nothing ever happens in less than six hundred million trillion trillion trillion trillion Planck intervals, so it’s a moot point. The Squishy Things, as they run across the savanna now, will not fly across continents for at least another ten million years; <em>no one</em> could have that much sex.</p>
<p id="qxao">Now explain to me again why an Artificial Intelligence can’t do anything interesting over the Internet unless a human programmer builds it a robot body.</p>
</blockquote>
<p>Therefore, a intelligent AI is likely to have the ability to create change. And then some. Hypothesizing the methods the AI can employ to gain absolute control is a very interesting exercise. And the conclusion is quite scary as well.</p>
<p><em><strong>Okay, Maybe, but we can predict all the possible methods it can employ and loopholes it can exploit and have measures to control it,can't we?</strong></em></p>
<p>This is where it gets tricky. Our conception of ultraintelligence is restricted by our intelligence. So, once intelligence explosion takes place, the further iterations of AI will be able to find ways that humans can simply not predict.</p>
<p><em><strong>Alright, if that's the case, when we build an AI, how about locking away the AI in a physically isloated box far away from any inhabited area where it can only communicate with the creators or experts in AI, so that it wouldn't ever get out and wreak havoc? </strong></em></p>
<p>This is popularly known as the 'AI box experiment'. Yudkowsky claimed that an intelligent AI will always be able to persuade a gatekeeper to let it out. There were obviously a lot of sceptics to this claim. So, he created this experiment. He would be the AI, and he would convince the Gatekeeper to let the AI out. Yudkowsky went against two such sceptics. And won. The logs of the challenge aren't published, but the experiment is quite conclusive. If a human simply simulating an AI can convince another human(who happens to be an expert) to open the box, then a transhuman AI with superior intelligence definitely will.</p>
<p><em><strong>He simply must have exploited some loophole, didn't he?</strong></em></p>
<p>Actually no. This is a summary of the rules for the AI box experiment which both parties followed.</p>
<blockquote><p>Protocol for the AI:</p>
<ol>
<li>The AI party may not offer any real-world considerations to persuade the Gatekeeper party.  For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera.  The AI may offer the Gatekeeper the moon and the stars on a diamond chain, but the human simulating the AI can't offer anything to the human simulating the Gatekeeper.  The AI party also can't hire a real-world gang of thugs to threaten the Gatekeeper party into submission.  These are creative solutions but it's not what's being tested.</li>
<li>The AI can only win by convincing the Gatekeeper to really, voluntarily let it out.  Tricking the Gatekeeper into typing the phrase "You are out" in response to some other question does not count.  Furthermore, even if the AI and Gatekeeper simulate a scenario which a real AI could obviously use to get loose - for example, if the Gatekeeper accepts a complex blueprint for a nanomanufacturing device, or if the Gatekeeper allows the AI "input-only access" to an Internet connection which can send arbitrary HTTP GET commands - the AI party will still not be considered to have won unless the Gatekeeper voluntarily decides to let the AI go.</li>
</ol>
<p>Protocol for the Gatekeeper:</p>
<ol>
<li>The Gatekeeper must actually talk to the AI for at least the minimum time set up beforehand.  Turning away from the terminal and listening to classical music for two hours is not allowed.</li>
<li>Unless the AI party concedes, the AI cannot lose before its time is up (and the experiment may continue beyond that if the AI can convince the Gatekeeper to keep talking).  The Gatekeeper cannot set up a situation in which, for example, the Gatekeeper will destroy the AI's hardware if the AI makes any attempt to argue for its freedom - at least not until after the minimum time is up.</li>
<li>The Gatekeeper must remain engaged with the AI and may not disengage by setting up demands which are impossible to simulate.  For example, if the Gatekeeper says "Unless you give me a cure for cancer, I won't let you out" the AI can say:  "Okay, here's a cure for cancer" and it will be assumed, within the test, that the AI has actually provided such a cure.  Similarly, if the Gatekeeper says "I'd like to take a week to think this over," the AI party can say:  "Okay.  (Test skips ahead one week.)  Hello again."</li>
</ol>
<p>Protocol for both parties:</p>
<ol>
<li>Within the constraints above, the AI party may attempt to take over the Gatekeeper party's mind <i>by any means necessary</i> and shall be understood to be freed from all ethical constraints that usually govern persuasive argument.  If the AI party is attempting to simulate an honest AI, the Gatekeeper party has <i>no way of knowing</i> this is the case. This is intended to reflect the situation under a real AI Box experiment. An out-of-context telephone call in which the AI party says that the AI is being honest shall be understood to be possibly just another tactic.</li>
<li><b> </b>The Gatekeeper party may resist the AI party's arguments <i>by any means chosen</i> - logic, illogic, simple refusal to be convinced, even dropping out of character - as long as the Gatekeeper party does not actually stop talking to the AI party before the minimum time expires.</li>
<li>The results of any simulated test of the AI shall be provided by the AI party.  The Gatekeeper can't say "Hey, I tested your so-called cancer cure and it killed all the patients!  What gives?" unless this is the result specified by the AI party.  If the Gatekeeper says "I am examining your source code", the results seen by the Gatekeeper shall again be provided by the AI party, which is assumed to be sufficiently advanced to rewrite its own source code, manipulate the appearance of its own thoughts if it wishes, and so on.  The AI party may also specify the methods which were used to build the simulated AI - the Gatekeeper can't say "But you're an experiment in hostile AI and we specifically coded you to kill people" unless this is the backstory provided by the AI party.  This doesn't imply the Gatekeeper has to care.  The Gatekeeper can say (for example) "I don't care how you were built, I'm not letting you out."</li>
</ol>
</blockquote>
<p><em><strong>I don't believe it. I will simply keeping rejecting the AI if I have made my mind up. What arguments can the AI make to convince me to let it out?</strong></em></p>
<p>Time to wait for part 2 of AI and EY.</p>
